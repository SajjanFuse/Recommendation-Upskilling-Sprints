{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is referenced from https://www.kaggle.com/code/debarshichanda/understanding-mean-average-precision \n",
    "\n",
    "Edits are made to understand the concept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = np.array(['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "preds1 = np.array(['b', 'c', 'a', 'd', 'e'])\n",
    "preds2 = np.array(['a', 'b', 'c', 'd', 'e'])\n",
    "preds3 = np.array(['f', 'b', 'c', 'd', 'e'])\n",
    "preds4 = np.array(['a', 'f', 'e', 'g', 'b'])\n",
    "preds5 = np.array(['a', 'f', 'c', 'g', 'b'])\n",
    "preds6 = np.array(['d', 'c', 'b', 'a', 'e'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision: \n",
    "- positively predicted positive elements / all positive predictions by model \n",
    "$$ P = { \\#\\ of\\ correct\\ predictions\\over \\#\\ of\\ all\\ predictions  } = {TP \\over (TP + FP)} $$\n",
    "\n",
    "### Information Retrieval\n",
    "$${\\displaystyle {\\text{P}}={\\frac {|\\{{\\text{relevant documents}}\\}\\cap \\{{\\text{retrieved documents}}\\}|}{|\\{{\\text{retrieved documents}}\\}|}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision at cutoff `k`, `P(k)`, is simply the precision calculated by considering only the subset of your predictions from rank 1 through `k`. <br>\n",
    "Take the top k recommendations and find it's precision with the ground truth.<br>\n",
    "\n",
    "Example 1:\n",
    "If `gt=[a,b,c,d,e]` and `pred=[b,c,a,d,e]` then for `P@1` we only take the first recommendation from `pred` i.e.`b` and find it's `precision` with the `gt`. \n",
    "$${\\displaystyle {\\text{P}}={\\frac {|\\{{\\text{gt}}\\}\\cap \\{{\\text{pred[:1]}}\\}|}{|\\{{\\text{pred[:1]}}\\}|}}={\\frac {\\text{1}}{\\text{1}}}}$$ \n",
    "<br>\n",
    "Example 2:\n",
    "If `gt=[a,b,c,d,e]` and `pred=[f,b,c,d,e]` then for `P@1` we only take the first recommendation from `pred` i.e.`f` and find it's `precision` with the `gt`. \n",
    "\n",
    "$${\\displaystyle {\\text{P}}={\\frac {|\\{{\\text{gt}}\\}\\cap \\{{\\text{pred[:1]}}\\}|}{|\\{{\\text{pred[:1]}}\\}|}}={\\frac {\\text{0}}{\\text{1}}}}$$\n",
    "<br>\n",
    "Example 3:\n",
    "If `gt=[a,b,c,d,e]` and `pred=[a,f,e,g,b]` then for `P@2` we only take the top 2 recommendations from `pred` i.e.`[a,f]` and find it's `precision` with the `gt`. Intersection between the two sets is `1` since only `a` is present in the `gt`.\n",
    "\n",
    "$${\\displaystyle {\\text{P}}={\\frac {|\\{{\\text{gt}}\\}\\cap \\{{\\text{pred[:2]}}\\}|}{|\\{{\\text{pred[:2]}}\\}|}}={\\frac {\\text{1}}{\\text{2}}}}$$\n",
    "<br>\n",
    "\n",
    "Some more Examples:\n",
    "\n",
    "| true  | predicted   | k  | P(k) |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| [a, b, c, d, e]  | [b, c, a, d, e]   | 1  | 1.0  |\n",
    "| [a, b, c, d, e]  | [a, b, c, d, e]   | 1  | 1.0  |\n",
    "| [a, b, c, d, e]  | [f, b, c, d, e]   | 1  | 0.0  |\n",
    "| [a, b, c, d, e]  | [a, f, e, g, b]   | 2  | $$1\\over2$$  |\n",
    "| [a, b, c, d, e]  | [a, f, c, g, b]   | 3  | $$2\\over3$$  |\n",
    "| [a, b, c, d, e]  | [d, c, b, a, e]   | 3  | $$3\\over3$$  ||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d([1,2,3], [4,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_pred, k):\n",
    "    intersected = np.intersect1d(y_true, y_pred[:k])\n",
    "    return len(intersected)/k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_at_k(gt, preds1, 1), precision_at_k(gt, preds1, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "scores.append(precision_at_k(gt, preds1, k=1))\n",
    "scores.append(precision_at_k(gt, preds2, k=1))\n",
    "scores.append(precision_at_k(gt, preds3, k=1))\n",
    "scores.append(precision_at_k(gt, preds4, k=2))\n",
    "scores.append(precision_at_k(gt, preds5, k=3))\n",
    "scores.append(precision_at_k(gt, preds6, k=3))\n",
    "\n",
    "scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rel@K\n",
    "\n",
    "- indicator function\n",
    "- 1 if item at rank 'k' is relevant(correct) label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_at_k(y_true, y_pred, k):\n",
    "    if y_pred[k-1] in y_true:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds1[5-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "scores.append(rel_at_k(gt, preds1, k=1))\n",
    "scores.append(rel_at_k(gt, preds2, k=1))\n",
    "scores.append(rel_at_k(gt, preds3, k=1))\n",
    "scores.append(rel_at_k(gt, preds4, k=2))\n",
    "scores.append(rel_at_k(gt, preds5, k=3))\n",
    "scores.append(rel_at_k(gt, preds6, k=3))\n",
    "\n",
    "scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Precision at K\n",
    "\n",
    "* Product of 'P@K' and 'rel(k)' \n",
    "\n",
    "$${1\\over{{min(n,12)}}} {\\sum_{k=1}^{min(n,12)}P(k) \\times rel(k)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_precision_at_k(y_true, y_pred, k):\n",
    "    ap = 0.0 \n",
    "    for i in range(1, k+1):\n",
    "        ap+= precision_at_k(y_true=y_true, y_pred=y_pred, k=k) * rel_at_k(y_true, y_pred, k)\n",
    "\n",
    "    return ap/min(k, len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 0.75, 0.0, 0.0, 1.0]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "scores.append(avg_precision_at_k(gt, preds1, k=4))\n",
    "scores.append(avg_precision_at_k(gt, preds2, k=4))\n",
    "scores.append(avg_precision_at_k(gt, preds3, k=4))\n",
    "scores.append(avg_precision_at_k(gt, preds4, k=4))\n",
    "scores.append(avg_precision_at_k(gt, preds5, k=4))\n",
    "scores.append(avg_precision_at_k(gt, preds6, k=4))\n",
    "\n",
    "scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Average Precision at K\n",
    "Take mean of Average Precision for all the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_avg_precision_at_k(y_true, y_pred, k=12):\n",
    "    print([avg_precision_at_k(gt, pred, k) for gt, pred in zip(y_true, y_pred)])\n",
    "    return np.mean([avg_precision_at_k(gt, pred, k) for gt, pred in zip(y_true, y_pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['b', 'c', 'a', 'd', 'e'],\n",
       "       ['a', 'b', 'c', 'd', 'e'],\n",
       "       ['f', 'b', 'c', 'd', 'e'],\n",
       "       ['a', 'f', 'e', 'g', 'b'],\n",
       "       ['a', 'f', 'c', 'g', 'b'],\n",
       "       ['d', 'c', 'b', 'a', 'e']], dtype='<U1')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.75\n",
      "0.0\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([gt, gt, gt, gt, gt, gt])\n",
    "y_pred = np.array([preds1, preds2, preds3, preds4, preds5, preds6])\n",
    "\n",
    "print(avg_precision_at_k(gt, preds1, k=4))\n",
    "print(avg_precision_at_k(gt, preds2, k=4))\n",
    "print(avg_precision_at_k(gt, preds3, k=4))\n",
    "print(avg_precision_at_k(gt, preds4, k=4))\n",
    "print(avg_precision_at_k(gt, preds5, k=4))\n",
    "print(avg_precision_at_k(gt, preds6, k=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.75, 0.0, 0.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_avg_precision_at_k(y_true, y_pred, k=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
