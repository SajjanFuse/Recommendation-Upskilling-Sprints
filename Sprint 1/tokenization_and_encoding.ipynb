{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "229e2deb-1457-4c00-8147-dbdb93226b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87b04692-1c6c-4fa0-850c-e46b6a3b6325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>lowercase_reviews</th>\n",
       "      <th>clean_reviews</th>\n",
       "      <th>reviews_sw_removed</th>\n",
       "      <th>reviews_wo_freq</th>\n",
       "      <th>stemmed_reviews</th>\n",
       "      <th>lemmatized_reviews</th>\n",
       "      <th>n_gram_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I admit, the great majority of films released ...</td>\n",
       "      <td>i admit, the great majority of films released ...</td>\n",
       "      <td>i admit the great majority of films released b...</td>\n",
       "      <td>admit great majority films released say 1933 d...</td>\n",
       "      <td>admit majority films released say 1933 dozen m...</td>\n",
       "      <td>admit major film releas say 1933 dozen major s...</td>\n",
       "      <td>admit major film releas say 1933 dozen major s...</td>\n",
       "      <td>[['admit', 'major', 'film'], ['major', 'film',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Take a low budget, inexperienced actors doubli...</td>\n",
       "      <td>take a low budget, inexperienced actors doubli...</td>\n",
       "      <td>take a low budget inexperienced actors doublin...</td>\n",
       "      <td>take low budget inexperienced actors doubling ...</td>\n",
       "      <td>take low budget inexperienced actors doubling ...</td>\n",
       "      <td>take low budget inexperienc actor doubl produc...</td>\n",
       "      <td>take low budget inexperienc actor doubl produc...</td>\n",
       "      <td>[['take', 'low', 'budget'], ['low', 'budget', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Everybody has seen 'Back To The Future,' right...</td>\n",
       "      <td>everybody has seen 'back to the future,' right...</td>\n",
       "      <td>everybody has seen back to the future right wh...</td>\n",
       "      <td>everybody seen back future right whether like ...</td>\n",
       "      <td>everybody seen back future right whether youve...</td>\n",
       "      <td>everybodi seen back futur right whether youv s...</td>\n",
       "      <td>everybodi seen back futur right whether youv s...</td>\n",
       "      <td>[['everybodi', 'seen', 'back'], ['seen', 'back...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doris Day was an icon of beauty in singing and...</td>\n",
       "      <td>doris day was an icon of beauty in singing and...</td>\n",
       "      <td>doris day was an icon of beauty in singing and...</td>\n",
       "      <td>doris day icon beauty singing acting warm voic...</td>\n",
       "      <td>doris day icon beauty singing acting warm voic...</td>\n",
       "      <td>dori day icon beauti sing act warm voic geniu ...</td>\n",
       "      <td>dori day icon beauti sing act warm voic geniu ...</td>\n",
       "      <td>[['dori', 'day', 'icon'], ['day', 'icon', 'bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After a series of silly, fun-loving movies, 19...</td>\n",
       "      <td>after a series of silly, fun-loving movies, 19...</td>\n",
       "      <td>after a series of silly funloving movies 1955 ...</td>\n",
       "      <td>series silly funloving movies 1955 big year do...</td>\n",
       "      <td>series silly funloving movies 1955 big year do...</td>\n",
       "      <td>seri silli funlov movi 1955 big year dori day ...</td>\n",
       "      <td>seri silli funlov movi 1955 big year dori day ...</td>\n",
       "      <td>[['seri', 'silli', 'funlov'], ['silli', 'funlo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  \\\n",
       "0  I admit, the great majority of films released ...   \n",
       "1  Take a low budget, inexperienced actors doubli...   \n",
       "2  Everybody has seen 'Back To The Future,' right...   \n",
       "3  Doris Day was an icon of beauty in singing and...   \n",
       "4  After a series of silly, fun-loving movies, 19...   \n",
       "\n",
       "                                   lowercase_reviews  \\\n",
       "0  i admit, the great majority of films released ...   \n",
       "1  take a low budget, inexperienced actors doubli...   \n",
       "2  everybody has seen 'back to the future,' right...   \n",
       "3  doris day was an icon of beauty in singing and...   \n",
       "4  after a series of silly, fun-loving movies, 19...   \n",
       "\n",
       "                                       clean_reviews  \\\n",
       "0  i admit the great majority of films released b...   \n",
       "1  take a low budget inexperienced actors doublin...   \n",
       "2  everybody has seen back to the future right wh...   \n",
       "3  doris day was an icon of beauty in singing and...   \n",
       "4  after a series of silly funloving movies 1955 ...   \n",
       "\n",
       "                                  reviews_sw_removed  \\\n",
       "0  admit great majority films released say 1933 d...   \n",
       "1  take low budget inexperienced actors doubling ...   \n",
       "2  everybody seen back future right whether like ...   \n",
       "3  doris day icon beauty singing acting warm voic...   \n",
       "4  series silly funloving movies 1955 big year do...   \n",
       "\n",
       "                                     reviews_wo_freq  \\\n",
       "0  admit majority films released say 1933 dozen m...   \n",
       "1  take low budget inexperienced actors doubling ...   \n",
       "2  everybody seen back future right whether youve...   \n",
       "3  doris day icon beauty singing acting warm voic...   \n",
       "4  series silly funloving movies 1955 big year do...   \n",
       "\n",
       "                                     stemmed_reviews  \\\n",
       "0  admit major film releas say 1933 dozen major s...   \n",
       "1  take low budget inexperienc actor doubl produc...   \n",
       "2  everybodi seen back futur right whether youv s...   \n",
       "3  dori day icon beauti sing act warm voic geniu ...   \n",
       "4  seri silli funlov movi 1955 big year dori day ...   \n",
       "\n",
       "                                  lemmatized_reviews  \\\n",
       "0  admit major film releas say 1933 dozen major s...   \n",
       "1  take low budget inexperienc actor doubl produc...   \n",
       "2  everybodi seen back futur right whether youv s...   \n",
       "3  dori day icon beauti sing act warm voic geniu ...   \n",
       "4  seri silli funlov movi 1955 big year dori day ...   \n",
       "\n",
       "                                      n_gram_reviews  \n",
       "0  [['admit', 'major', 'film'], ['major', 'film',...  \n",
       "1  [['take', 'low', 'budget'], ['low', 'budget', ...  \n",
       "2  [['everybodi', 'seen', 'back'], ['seen', 'back...  \n",
       "3  [['dori', 'day', 'icon'], ['day', 'icon', 'bea...  \n",
       "4  [['seri', 'silli', 'funlov'], ['silli', 'funlo...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('1000_lemmatized.csv')\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1507d11b-a618-42ce-9655-ced96268b1ba",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "- splitting raw text into small chunks of words or sentences\n",
    "- text to words : word tokenizer\n",
    "- text to sentences : sentence tokenizer\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "efeed0e0-b9a9-4681-97d3-69fe49ff1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "temp_text = df[\"lemmatized_reviews\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01a0e398-197c-44d8-aa29-37b2e9d420a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admit major film releas say 1933 dozen major silent view love crowd two last command citi light latter chaplin circa 1931br apprehens humor often difficult appreci uh enjoy decad later lead actor thought littl filmbr intrigu sequenc earli guy suppos get delous three minut fulli dress schtick background perhap three dozen men pas nake white black wwi butt part full backsid shown earli variat beefcak courtesi howard hugh\n"
     ]
    }
   ],
   "source": [
    "print(temp_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a01b21c-1209-45e6-8913-4286c35c2101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "import tensorflow as tf \n",
    "import keras.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0227b5c-055b-4518-88e6-81e59d5d666a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK tokenizer tokenized as: ['admit', 'major', 'film', 'releas', 'say', '1933', 'dozen', 'major', 'silent', 'view', 'love', 'crowd', 'two', 'last', 'command', 'citi', 'light', 'latter', 'chaplin', 'circa']\n",
      "\n",
      "Keras tokenizer tokenized as: ['admit', 'major', 'film', 'releas', 'say', '1933', 'dozen', 'major', 'silent', 'view', 'love', 'crowd', 'two', 'last', 'command', 'citi', 'light', 'latter', 'chaplin', 'circa']\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tokens_tf = tf.keras.preprocessing.text.text_to_word_sequence(temp_text) \n",
    "tokens_nltk = word_tokenize(temp_text) \n",
    "print(f'NLTK tokenizer tokenized as: {tokens_nltk[:20]}\\n')\n",
    "print(f'Keras tokenizer tokenized as: {tokens_tf[:20]}\\n')\n",
    "print(tokens_nltk==tokens_tf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d3cfedd3-dc2d-42b3-b94b-33c481840f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK tokenizer tokenized as: ['[', '[', \"'admit\", \"'\", ',', \"'major\", \"'\", ',', \"'film\", \"'\", ']', ',', '[', \"'major\", \"'\", ',', \"'film\", \"'\", ',', \"'releas\"]\n",
      "\n",
      "Keras tokenizer tokenized as: [\"'admit'\", \"'major'\", \"'film'\", \"'major'\", \"'film'\", \"'releas'\", \"'film'\", \"'releas'\", \"'say'\", \"'releas'\", \"'say'\", \"'1933'\", \"'say'\", \"'1933'\", \"'dozen'\", \"'1933'\", \"'dozen'\", \"'major'\", \"'dozen'\", \"'major'\"]\n",
      "\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# trying for the n-gram reviews \n",
    "temp_text2 = df[\"n_gram_reviews\"][0]\n",
    "\n",
    "tokens_tf = tf.keras.preprocessing.text.text_to_word_sequence(temp_text2) \n",
    "tokens_nltk = word_tokenize(temp_text2) \n",
    "print(f'NLTK tokenizer tokenized as: {tokens_nltk[:20]}\\n')\n",
    "print(f'Keras tokenizer tokenized as: {tokens_tf[:20]}\\n')\n",
    "print(tokens_nltk==tokens_tf) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f44022-01db-45bd-8712-8c10dd1a5c41",
   "metadata": {},
   "source": [
    "## Thus, tokenizer only separates the words. No matter the occurence, all the occurences are counted and displayed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b29f69f7-8d9b-4abb-9d83-7eb97727c196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>lowercase_reviews</th>\n",
       "      <th>clean_reviews</th>\n",
       "      <th>reviews_sw_removed</th>\n",
       "      <th>reviews_wo_freq</th>\n",
       "      <th>stemmed_reviews</th>\n",
       "      <th>lemmatized_reviews</th>\n",
       "      <th>n_gram_reviews</th>\n",
       "      <th>tokenized_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I admit, the great majority of films released ...</td>\n",
       "      <td>i admit, the great majority of films released ...</td>\n",
       "      <td>i admit the great majority of films released b...</td>\n",
       "      <td>admit great majority films released say 1933 d...</td>\n",
       "      <td>admit majority films released say 1933 dozen m...</td>\n",
       "      <td>admit major film releas say 1933 dozen major s...</td>\n",
       "      <td>admit major film releas say 1933 dozen major s...</td>\n",
       "      <td>[['admit', 'major', 'film'], ['major', 'film',...</td>\n",
       "      <td>[admit, major, film, releas, say, 1933, dozen,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Take a low budget, inexperienced actors doubli...</td>\n",
       "      <td>take a low budget, inexperienced actors doubli...</td>\n",
       "      <td>take a low budget inexperienced actors doublin...</td>\n",
       "      <td>take low budget inexperienced actors doubling ...</td>\n",
       "      <td>take low budget inexperienced actors doubling ...</td>\n",
       "      <td>take low budget inexperienc actor doubl produc...</td>\n",
       "      <td>take low budget inexperienc actor doubl produc...</td>\n",
       "      <td>[['take', 'low', 'budget'], ['low', 'budget', ...</td>\n",
       "      <td>[take, low, budget, inexperienc, actor, doubl,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Everybody has seen 'Back To The Future,' right...</td>\n",
       "      <td>everybody has seen 'back to the future,' right...</td>\n",
       "      <td>everybody has seen back to the future right wh...</td>\n",
       "      <td>everybody seen back future right whether like ...</td>\n",
       "      <td>everybody seen back future right whether youve...</td>\n",
       "      <td>everybodi seen back futur right whether youv s...</td>\n",
       "      <td>everybodi seen back futur right whether youv s...</td>\n",
       "      <td>[['everybodi', 'seen', 'back'], ['seen', 'back...</td>\n",
       "      <td>[everybodi, seen, back, futur, right, whether,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doris Day was an icon of beauty in singing and...</td>\n",
       "      <td>doris day was an icon of beauty in singing and...</td>\n",
       "      <td>doris day was an icon of beauty in singing and...</td>\n",
       "      <td>doris day icon beauty singing acting warm voic...</td>\n",
       "      <td>doris day icon beauty singing acting warm voic...</td>\n",
       "      <td>dori day icon beauti sing act warm voic geniu ...</td>\n",
       "      <td>dori day icon beauti sing act warm voic geniu ...</td>\n",
       "      <td>[['dori', 'day', 'icon'], ['day', 'icon', 'bea...</td>\n",
       "      <td>[dori, day, icon, beauti, sing, act, warm, voi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After a series of silly, fun-loving movies, 19...</td>\n",
       "      <td>after a series of silly, fun-loving movies, 19...</td>\n",
       "      <td>after a series of silly funloving movies 1955 ...</td>\n",
       "      <td>series silly funloving movies 1955 big year do...</td>\n",
       "      <td>series silly funloving movies 1955 big year do...</td>\n",
       "      <td>seri silli funlov movi 1955 big year dori day ...</td>\n",
       "      <td>seri silli funlov movi 1955 big year dori day ...</td>\n",
       "      <td>[['seri', 'silli', 'funlov'], ['silli', 'funlo...</td>\n",
       "      <td>[seri, silli, funlov, movi, 1955, big, year, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  \\\n",
       "0  I admit, the great majority of films released ...   \n",
       "1  Take a low budget, inexperienced actors doubli...   \n",
       "2  Everybody has seen 'Back To The Future,' right...   \n",
       "3  Doris Day was an icon of beauty in singing and...   \n",
       "4  After a series of silly, fun-loving movies, 19...   \n",
       "\n",
       "                                   lowercase_reviews  \\\n",
       "0  i admit, the great majority of films released ...   \n",
       "1  take a low budget, inexperienced actors doubli...   \n",
       "2  everybody has seen 'back to the future,' right...   \n",
       "3  doris day was an icon of beauty in singing and...   \n",
       "4  after a series of silly, fun-loving movies, 19...   \n",
       "\n",
       "                                       clean_reviews  \\\n",
       "0  i admit the great majority of films released b...   \n",
       "1  take a low budget inexperienced actors doublin...   \n",
       "2  everybody has seen back to the future right wh...   \n",
       "3  doris day was an icon of beauty in singing and...   \n",
       "4  after a series of silly funloving movies 1955 ...   \n",
       "\n",
       "                                  reviews_sw_removed  \\\n",
       "0  admit great majority films released say 1933 d...   \n",
       "1  take low budget inexperienced actors doubling ...   \n",
       "2  everybody seen back future right whether like ...   \n",
       "3  doris day icon beauty singing acting warm voic...   \n",
       "4  series silly funloving movies 1955 big year do...   \n",
       "\n",
       "                                     reviews_wo_freq  \\\n",
       "0  admit majority films released say 1933 dozen m...   \n",
       "1  take low budget inexperienced actors doubling ...   \n",
       "2  everybody seen back future right whether youve...   \n",
       "3  doris day icon beauty singing acting warm voic...   \n",
       "4  series silly funloving movies 1955 big year do...   \n",
       "\n",
       "                                     stemmed_reviews  \\\n",
       "0  admit major film releas say 1933 dozen major s...   \n",
       "1  take low budget inexperienc actor doubl produc...   \n",
       "2  everybodi seen back futur right whether youv s...   \n",
       "3  dori day icon beauti sing act warm voic geniu ...   \n",
       "4  seri silli funlov movi 1955 big year dori day ...   \n",
       "\n",
       "                                  lemmatized_reviews  \\\n",
       "0  admit major film releas say 1933 dozen major s...   \n",
       "1  take low budget inexperienc actor doubl produc...   \n",
       "2  everybodi seen back futur right whether youv s...   \n",
       "3  dori day icon beauti sing act warm voic geniu ...   \n",
       "4  seri silli funlov movi 1955 big year dori day ...   \n",
       "\n",
       "                                      n_gram_reviews  \\\n",
       "0  [['admit', 'major', 'film'], ['major', 'film',...   \n",
       "1  [['take', 'low', 'budget'], ['low', 'budget', ...   \n",
       "2  [['everybodi', 'seen', 'back'], ['seen', 'back...   \n",
       "3  [['dori', 'day', 'icon'], ['day', 'icon', 'bea...   \n",
       "4  [['seri', 'silli', 'funlov'], ['silli', 'funlo...   \n",
       "\n",
       "                                   tokenized_reviews  \n",
       "0  [admit, major, film, releas, say, 1933, dozen,...  \n",
       "1  [take, low, budget, inexperienc, actor, doubl,...  \n",
       "2  [everybodi, seen, back, futur, right, whether,...  \n",
       "3  [dori, day, icon, beauti, sing, act, warm, voi...  \n",
       "4  [seri, silli, funlov, movi, 1955, big, year, d...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokenized_reviews\"] = df[\"lemmatized_reviews\"].apply(lambda text:tf.keras.preprocessing.text.text_to_word_sequence(text))\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271e404-b8a9-4053-89a3-b010c44e28f5",
   "metadata": {},
   "source": [
    "# Encoding \n",
    "\n",
    "- converting the words into numbers/vectors\n",
    "    - to preserve context and relationship between words and sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b62c6-d686-40ea-9758-b60d437422c1",
   "metadata": {},
   "source": [
    "## Index-Based Encoding \n",
    "\n",
    "- assign unique index to each token in the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62fa49ff-e041-483c-8e0c-922024203d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115169 13430\n"
     ]
    }
   ],
   "source": [
    "# getting all the vocabulary from all the reviews \n",
    "all_words = [] \n",
    "vocab = []\n",
    "for index, row in df.iterrows():\n",
    "    all_words.extend(row['tokenized_reviews'])\n",
    "    for word in row['tokenized_reviews']:\n",
    "        if(word) not in vocab:\n",
    "            vocab.append(word) \n",
    "print(len(all_words), len(vocab)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5e05e40-474c-464e-b22b-5365765dc187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.575502606105733\n"
     ]
    }
   ],
   "source": [
    "print(len(all_words)/len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd3146fe-31e4-468e-b903-af7b9a9ca10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['intrigu', 'sequenc', 'earli', 'guy', 'suppos']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[34:39] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dd59e003-4d87-4890-af2e-93a5229cff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word to index encoding \n",
    "word_to_index = {word:index for index, word in enumerate(vocab)} \n",
    "# word_to_index.items()[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e118f7-b970-4fd7-8bc1-accb3923dd79",
   "metadata": {},
   "source": [
    "## Bag of Words (BOW) \n",
    "\n",
    "- describes occurences of words within a document\n",
    "- only concerned with whether known words occur in the document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8ae3f1ba-d91d-479d-909e-d0d507130156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(text):\n",
    "    # initializing vector of zeros with same length as vocab \n",
    "    normal_bow_vector = [0] * len(vocab) \n",
    "    binary_bow_vector = [0] * len(vocab)\n",
    "    words = text.split() \n",
    "    for word in words:\n",
    "        if word in word_to_index:\n",
    "            #normal bow takes the overall count of the words \n",
    "            normal_bow_vector[word_to_index[word]]+=1\n",
    "            # binary bow changes to 1 if present, 0 if absent \n",
    "            binary_bow_vector[word_to_index[word]] = 1\n",
    "    return normal_bow_vector, binary_bow_vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dffc7a12-cdfa-48d9-8caa-32a68fb19bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 1, 1]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow(\"admit major film releas say 1933 dozen major silent butt part full\")[0][:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "814e6231-cce1-470e-8b69-22247d2d648f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow(\"admit major film releas say 1933 dozen major silent butt part full\")[0][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "41e53081-3bfd-4eed-b258-547db2b58054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# using library \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = \"admit major film releas say 1933 dozen major silent butt part full\"\n",
    "vectorizer_normal = CountVectorizer(vocabulary=vocab)\n",
    "vectorizer_binary = CountVectorizer(vocabulary=vocab, binary=True) \n",
    "\n",
    "binary_bow_matrix = vectorizer_binary.fit_transform([text])\n",
    "normal_bow_matrix = vectorizer_normal.fit_transform([text]) \n",
    "print(type(binary_bow_matrix.toarray()), type(binary_bow_matrix.toarray())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e85ad70f-0276-4168-9a15-5aafdb8916e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 13430), (1, 13430))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_bow_matrix.shape, binary_bow_matrix.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "81de4886-3a01-428a-bd74-ac8d86da45e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13430,), (1, 13430))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_bow_scratch, normal_bow_scratch = bow(text)\n",
    "np.array(binary_bow_scratch).shape, binary_bow_matrix.toarray().shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1f658de3-5797-4c77-a6e4-21370f15b16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, ..., 0, 0, 0]], dtype=int64), [1, 2, 1, 1, 1])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_bow_matrix.toarray(), binary_bow_scratch[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5db667-aba7-42d7-98af-c93736aa20b9",
   "metadata": {},
   "source": [
    "## TF_IDF Encoding\n",
    "\n",
    "- every word has relative frequency coding with respect to current sentence and whole document\n",
    "- Term Frequency : occurence of current word in current sentence\n",
    "    $$ TF = \\frac{Number \\, of \\, times \\, word \\, appears\\, in \\, the \\, document}{Total\\, number\\, of\\, words\\, in\\, the\\, document} $$\n",
    "\n",
    "- Inverse Data Frequency : log of total number of words in whole data with respect to total number of sentences containing the current word\n",
    "\n",
    "$$ IDF = log (\\frac {Total\\,number\\,of\\,documents\\,in\\,the\\,corpus}{Number\\,of\\,documents\\,containing\\,the\\,word}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6a04ce4b-51c4-4184-a33e-fb06d8e4f1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.42615959880289433\n",
      "  (0, 9)\t0.42615959880289433\n",
      "  (0, 8)\t0.42615959880289433\n",
      "  (0, 4)\t0.42615959880289433\n",
      "  (0, 6)\t0.3032160644503863\n",
      "  (0, 1)\t0.42615959880289433\n",
      "  (1, 5)\t0.42615959880289433\n",
      "  (1, 7)\t0.42615959880289433\n",
      "  (1, 2)\t0.42615959880289433\n",
      "  (1, 10)\t0.42615959880289433\n",
      "  (1, 3)\t0.42615959880289433\n",
      "  (1, 6)\t0.3032160644503863\n",
      "feature_names ['1933' 'admit' 'butt' 'dozen' 'film' 'full' 'major' 'part' 'releas' 'say'\n",
      " 'silent']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1933</th>\n",
       "      <th>admit</th>\n",
       "      <th>butt</th>\n",
       "      <th>dozen</th>\n",
       "      <th>film</th>\n",
       "      <th>full</th>\n",
       "      <th>major</th>\n",
       "      <th>part</th>\n",
       "      <th>releas</th>\n",
       "      <th>say</th>\n",
       "      <th>silent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.303216</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.303216</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.42616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1933    admit     butt    dozen     film     full     major     part  \\\n",
       "0  0.42616  0.42616  0.00000  0.00000  0.42616  0.00000  0.303216  0.00000   \n",
       "1  0.00000  0.00000  0.42616  0.42616  0.00000  0.42616  0.303216  0.42616   \n",
       "\n",
       "    releas      say   silent  \n",
       "0  0.42616  0.42616  0.00000  \n",
       "1  0.00000  0.00000  0.42616  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without the vocabulary passed to the vectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text2 = [\"admit major film releas say 1933\" ,\"dozen major silent butt part full\"]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text2)\n",
    "print(tfidf_matrix) \n",
    "# Stopwords get removed automatically\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"feature_names\",feature_names)\n",
    "\n",
    "tfidf_df = pd.DataFrame(data=tfidf_matrix.toarray(), columns=feature_names)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab3ada-0c3f-4abf-b6ea-eef54a6a5618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2badfc86-f664-44a2-8757-348d3028a385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admit major film releas say 1933 dozen major silent butt part full feature_names ['admit' 'major' 'film' ... 'preconceiv' 'phenom' 'wayyyi']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(vocabulary=vocab) \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
    "\n",
    "# Stopwords get removed automatically\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(text, \"feature_names\",feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "50d676a7-d393-42c9-a440-b17fd45e5509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>major</th>\n",
       "      <th>film</th>\n",
       "      <th>releas</th>\n",
       "      <th>say</th>\n",
       "      <th>1933</th>\n",
       "      <th>dozen</th>\n",
       "      <th>silent</th>\n",
       "      <th>view</th>\n",
       "      <th>love</th>\n",
       "      <th>...</th>\n",
       "      <th>merchant</th>\n",
       "      <th>2br</th>\n",
       "      <th>pube</th>\n",
       "      <th>crossroad</th>\n",
       "      <th>emmenth</th>\n",
       "      <th>vienna</th>\n",
       "      <th>womenbr</th>\n",
       "      <th>preconceiv</th>\n",
       "      <th>phenom</th>\n",
       "      <th>wayyyi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.267261</td>\n",
       "      <td>0.534522</td>\n",
       "      <td>0.267261</td>\n",
       "      <td>0.267261</td>\n",
       "      <td>0.267261</td>\n",
       "      <td>0.267261</td>\n",
       "      <td>0.267261</td>\n",
       "      <td>0.267261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 13430 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      admit     major      film    releas       say      1933     dozen  \\\n",
       "0  0.267261  0.534522  0.267261  0.267261  0.267261  0.267261  0.267261   \n",
       "\n",
       "     silent  view  love  ...  merchant  2br  pube  crossroad  emmenth  vienna  \\\n",
       "0  0.267261   0.0   0.0  ...       0.0  0.0   0.0        0.0      0.0     0.0   \n",
       "\n",
       "   womenbr  preconceiv  phenom  wayyyi  \n",
       "0      0.0         0.0     0.0     0.0  \n",
       "\n",
       "[1 rows x 13430 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(data=tfidf_matrix.toarray(), columns=feature_names)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f4db3fe3-d889-4267-a15a-0f4e60d9437d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.099089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.000000  0.166667  0.166667  0.000000  0.166667  0.000000  0.000000   \n",
       "1  0.166667  0.000000  0.000000  0.166667  0.000000  0.166667  0.166667   \n",
       "\n",
       "         7         8         9         10  \n",
       "0  0.000000  0.166667  0.166667  0.099089  \n",
       "1  0.166667  0.000000  0.000000  0.099089  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def calculate_tf(text, vocab):\n",
    "    tf_vector = np.zeros(len(vocab))\n",
    "    tokens = tokenize(text)\n",
    "    total_words = len(tokens)\n",
    "    for word in tokens:\n",
    "        if word in vocab:\n",
    "            tf_vector[vocab.index(word)] += 1 / total_words\n",
    "    return tf_vector\n",
    "\n",
    "def calculate_idf(docs, vocab):\n",
    "    idf_vector = np.zeros(len(vocab))\n",
    "    total_docs = len(docs)\n",
    "    for word in vocab:\n",
    "        doc_count = sum(1 for doc in docs if word in doc)\n",
    "        idf_vector[vocab.index(word)] = np.log(total_docs / (doc_count + 1)) + 1\n",
    "    return idf_vector\n",
    "\n",
    "def tfidf_vect(docs):\n",
    "    all_words = set()\n",
    "    for doc in docs:\n",
    "        all_words.update(tokenize(doc))\n",
    "    vocab = list(all_words)\n",
    "    \n",
    "    tf_matrix = np.array([calculate_tf(doc, vocab) for doc in docs])\n",
    "    idf_vector = calculate_idf(docs, vocab)\n",
    "    \n",
    "    tfidf_matrix = tf_matrix * idf_vector\n",
    "    \n",
    "    return tfidf_matrix, vocab\n",
    "\n",
    "tfidf_matrix, vocab = tfidf_vect(text2)\n",
    "tfidf_df = pd.DataFrame(data=tfidf_matrix)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e48b46-4bfd-4610-b763-8e8b58391e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b26452-98df-4f31-806c-6512d874093b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
